{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Authentication**"
      ],
      "metadata": {
        "id": "zIEiq4W23nAG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tweepy\n",
        "from textblob import TextBlob\n",
        "\n",
        "\n",
        "\n",
        "consumer_key ='C3n0EIBkRhpM6EohRja4wBxEH'\n",
        "consumer_key_secret ='Uu127IPY1R8yQT2klE4WAD4qsK9DKDaahptvSpDOq2gkib9pth'\n",
        "access_token ='954075172264431616-v8ldAac4R4XJguYwja6nVhY0zzP25Wi'\n",
        "access_token_secret='5JXnXe3YHD4FC5gC6USU4rtRNsif7Xp6wTarE7uYHF6zP'\n",
        "\n",
        "\n",
        "# Priyanks access ->\n",
        "# consumer_key = 'ZqvGZgMWusmV8cVYekYAzD1CJ'\n",
        "# consumer_key_secret = 'iR2QptF7WfMlrdFdtCPPEmlyQWdx4qEMUd6IvtpArXLGmIqETe'\n",
        "# access_token = '1602361115349917697-Gw1Fx4hEqSBGlM8kx0eXMwf8nytpfg'\n",
        "# access_token_secret = 'IfsH8uSVFqWCvEUwtMHH9zR54vBl3oF0JRXthLH9Vtfo9'\n",
        "\n",
        "auth = tweepy.OAuthHandler(consumer_key, consumer_key_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "\n",
        "api = tweepy.API(auth)\n"
      ],
      "metadata": {
        "id": "JOcDFTf65oNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Fetch the real time Data**"
      ],
      "metadata": {
        "id": "a6qoqXza3NEj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def search(query, count):\n",
        "    df_tweets = pd.DataFrame(columns = ['username','retweetcount', 'tweet'])\n",
        "    try:\n",
        "      tweets_list = [tweet for tweet in tweepy.Cursor(api.search, q=query, lang=\"en\", tweet_mode='extended').items(count)]\n",
        "      for tweet in tweets_list:\n",
        "          username = tweet.user.screen_name\n",
        "          retweetcount = tweet.retweet_count\n",
        "          try:\n",
        "              tweet = tweet.retweeted_status.full_text\n",
        "          except AttributeError:  \n",
        "              tweet = tweet.full_text\n",
        "          df_tweets.loc[len(df_tweets)] = [username, retweetcount, tweet]\n",
        "    except tweepy.TweepError as e:\n",
        "          print(f\"Error: Twitter Authentication Failed - \\n{str(e)}\") \n",
        "    return df_tweets"
      ],
      "metadata": {
        "id": "5s9ekhouForA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = search(\"elon\", 800)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GbZT_EYoFwO6",
        "outputId": "51abcac1-ffb5-4603-f952-430bdb0292ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: Twitter Authentication Failed - \n",
            "Twitter error response: status code = 401\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()\n",
        "df.head()"
      ],
      "metadata": {
        "id": "2Fag3V6EI1WD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Preprocessing**"
      ],
      "metadata": {
        "id": "-mr-c8dExXiE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tweet-preprocessor"
      ],
      "metadata": {
        "id": "hhaQ5lIHDSQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df.info()\n",
        "\n",
        "df=df.drop_duplicates(subset=['tweet'], keep=False)\n",
        "\n",
        "df.info()\n"
      ],
      "metadata": {
        "id": "hnRVKCwrPTw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import preprocessor as p\n",
        "# URL,  Mentions are removed\n",
        "df['clean_tweet1'] = df['tweet'].apply(lambda x : p.clean(x))\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "rwL8pZ4yHoLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['clean_tweet2'] = df['clean_tweet1'].str.replace(\"[^a-zA-Z# ]\", '' ,regex=True )\n",
        "#punctuations are removed\n",
        "df.head()"
      ],
      "metadata": {
        "id": "YQ2xdO5JkBko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stopWords = set(stopwords.words(\"english\"))\n",
        "\n",
        "#remove stop words\n",
        "df['most_clean_tweet'] = df['clean_tweet2'].apply(lambda x: ' '.join([item for item in x.split() for j in item.split() if j not in stopWords ]))\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "oz3tcv3RpEWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "clean_tweet3 was the most processed tweet.\n",
        "\n"
      ],
      "metadata": {
        "id": "jwk6n19U2Tku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df['polarity'] = df['most_clean_tweet'].apply(lambda x : TextBlob(x).sentiment.polarity)\n",
        "\n",
        "df['Sentiment'] = df['polarity'].apply(lambda x : 'negative' if x < 0 else 'positive' if x >0 else 'neutral')\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "neTbTIXUbfNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model**"
      ],
      "metadata": {
        "id": "jIwS_rYvzXXg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_variable = df['Sentiment'].apply(lambda x: 0 if x=='negative' else 1 if x=='positive' else -1 )"
      ],
      "metadata": {
        "id": "uAP61ftPFM2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# bag-of-words vectorised matrix\n",
        "bow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, stop_words='english', max_features=150)\n",
        "bow_word_feature = bow_vectorizer.fit_transform(df['most_clean_tweet'])\n",
        "\n",
        "#get_feature_names_out provides the uniques words of the vocabulary\n",
        "print(bow_vectorizer.get_feature_names_out())\n",
        "\n",
        "#frequency of each word within the document\n",
        "bow_word_feature.todense()\n",
        "\n",
        "X_trainBW, X_testBW, y_trainBW, y_testBW = train_test_split(bow_word_feature, target_variable, test_size=0.25, random_state=3342)\n"
      ],
      "metadata": {
        "id": "KhtNFhgWxUIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.feature_extraction.text import  TfidfVectorizer\n",
        "# Tfid vectorised matrix\n",
        "\n",
        "Tfid_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, stop_words='english', max_features=150)\n",
        "Tfid_feature = Tfid_vectorizer.fit_transform(df['most_clean_tweet'])\n",
        "\n",
        "print(Tfid_vectorizer.get_feature_names_out())\n",
        "Tfid_feature.todense()\n",
        "\n",
        "X_trainTF, X_testTF, y_trainTF, y_testTF = train_test_split(Tfid_feature, target_variable, test_size=0.25, random_state=3342)\n"
      ],
      "metadata": {
        "id": "Am63avcvF7ZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn import metrics\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "\n",
        "def processModel(classifier, X_train, X_test, y_train, y_test):\n",
        "    classifier.fit(X_train.toarray(), y_train)\n",
        "    # predictions\n",
        "    predictions = classifier.predict(X_test.toarray())\n",
        "    # calculating accuracy\n",
        "    accuracy=metrics.accuracy_score(predictions,y_test)\n",
        "    return accuracy;"
      ],
      "metadata": {
        "id": "wcJGKu4CYn5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
        "\n",
        "DecisionTreeClassifier_BOW = processModel(classifier, X_trainBW, X_testBW, y_trainBW, y_testBW)*100\n",
        "\n",
        "print('DecisionTreeClassifier_BOW Accuracy Score : ', DecisionTreeClassifier_BOW)\n",
        "\n",
        "DecisionTreeClassifier_TF_IDF = processModel(classifier, X_trainTF, X_testTF, y_trainTF, y_testTF )*100\n",
        "\n",
        "print('DecisionTreeClassifier_TF_IDF Accuracy Score : ', DecisionTreeClassifier_TF_IDF)\n"
      ],
      "metadata": {
        "id": "KjoeG2tMUGwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "classifier = RandomForestClassifier(n_estimators = 150, criterion = 'entropy', random_state = 0)\n",
        "\n",
        "RandomForestClassifier_BOW = processModel(classifier, X_trainBW, X_testBW, y_trainBW, y_testBW)*100\n",
        "\n",
        "print('RandomForestClassifier_BOW Accuracy Score : ', RandomForestClassifier_BOW)\n",
        "\n",
        "RandomForestClassifier_TF_IDF = processModel(classifier, X_trainTF, X_testTF, y_trainTF, y_testTF )*100\n",
        "\n",
        "print('RandomForestClassifier_TF_IDF Accuracy Score : ', RandomForestClassifier_TF_IDF)"
      ],
      "metadata": {
        "id": "U288rFAzXBBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "naive_classifier = GaussianNB()\n",
        "\n",
        "GuassionNaiveBayes_BOW = processModel(naive_classifier, X_trainBW, X_testBW, y_trainBW, y_testBW)*100\n",
        "\n",
        "print('GuassionNaiveBayes_BOW Accuracy Score : ', GuassionNaiveBayes_BOW)\n",
        "\n",
        "GuassionNaiveBayes_TF_IDF = processModel(naive_classifier, X_trainTF, X_testTF, y_trainTF, y_testTF )*100\n",
        "\n",
        "print('GuassionNaiveBayes_TF_IDF Accuracy Score : ', GuassionNaiveBayes_TF_IDF)"
      ],
      "metadata": {
        "id": "sIu-Pf_-a2BM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Comparision**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oma12RHoObPX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "DecisionTreeClassifierData = {'DecisionTreeClassifier - BOW ': DecisionTreeClassifier_BOW, 'DecisionTreeClassifier - TF_IDF': DecisionTreeClassifier_TF_IDF}\n",
        "\n",
        "RandomForestClassifierData = {'RandomForestClassifier - BOW': RandomForestClassifier_BOW, 'RandomForestClassifier - TF_IDF': RandomForestClassifier_TF_IDF}\n",
        "                \n",
        "NaiveBayesData = { 'Naive Bayes - BOW':GuassionNaiveBayes_BOW, 'Naive Bayes - TF_IDF':GuassionNaiveBayes_TF_IDF }\n",
        "\n",
        "\n",
        "fig = plt.figure(figsize = (20, 8))\n",
        " \n",
        "plt.bar(list(DecisionTreeClassifierData.keys()), list(DecisionTreeClassifierData.values()), color ='maroon', width = 0.5)\n",
        "plt.bar(list(RandomForestClassifierData.keys()), list(RandomForestClassifierData.values()), color ='royalblue', width = 0.5)\n",
        "plt.bar(list(NaiveBayesData.keys()), list(NaiveBayesData.values()), color ='mediumseagreen', width = 0.5)\n",
        " \n",
        "plt.xlabel(\"Models\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Accuracy of models\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wbbwDAzPNnBk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}